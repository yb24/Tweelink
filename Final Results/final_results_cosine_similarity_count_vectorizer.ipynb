{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_results_cosine_similarity_count_vectorizer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ts-fOnw4P9WO",
        "Ua-dlovqU2t6",
        "bW6BjAZVUyuq",
        "DsKQrbhRUptK",
        "Ty_NvU4DuLSn",
        "v4sF3KZuvSuV",
        "IK7PnL_lwBwU",
        "jQ-zx_PMVMBU",
        "rg9trzhdY5c-",
        "YFQIDpqvaTYn",
        "zxyJ3ooRzXwW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Statements"
      ],
      "metadata": {
        "id": "Ts-fOnw4P9WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "from itertools import islice\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import nltk\n",
        "from glob import glob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import math\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('corpus')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import stats\n",
        "! pip install git+https://github.com/LIAAD/yake\n",
        "import yake\n",
        "! pip install multi_rake\n",
        "from multi_rake import Rake\n",
        "! pip install summa\n",
        "from summa import keywords as summa_keywords\n",
        "! pip install keybert\n",
        "from keybert import KeyBERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBOv1bdeQBSj",
        "outputId": "738057d9-9ebb-4861-be1c-f26684b4a3f6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n",
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-ckb15b4c\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-ckb15b4c\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.8.9)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.21.6)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (2.6.3)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.7/dist-packages (from yake==0.4.8) (0.9.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake==0.4.8) (2019.12.20)\n",
            "Requirement already satisfied: multi_rake in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.4 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (1.21.6)\n",
            "Requirement already satisfied: regex>=2018.6.6 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (2019.12.20)\n",
            "Requirement already satisfied: pyrsistent>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (0.18.1)\n",
            "Requirement already satisfied: pycld2>=0.41 in /usr/local/lib/python3.7/dist-packages (from multi_rake) (0.41)\n",
            "Requirement already satisfied: summa in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.21.6)\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.21.6)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.7/dist-packages (from keybert) (2.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keybert) (1.0.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.7/dist-packages (from keybert) (12.3.0)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.4.0->keybert) (4.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keybert) (1.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.18.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.5.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.11.0+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.12.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.64.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.2.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.0.49)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSb0ZbaNQHKn",
        "outputId": "09d34bed-534d-40de-f31e-25c7a8c76258"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TyRRoA3QJ-e",
        "outputId": "d47db44a-0ff5-4cf5-9203-9f8c2d4993d9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open(\"/content/drive/MyDrive/Tweelink_Dataset/twitter_base_preprocessed.pkl\", \"rb\")\n",
        "df = pickle.load(file1)\n",
        "file1.close()"
      ],
      "metadata": {
        "id": "wZK40Dw9QLda"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input"
      ],
      "metadata": {
        "id": "Ua-dlovqU2t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u_base_hashtag = input(\"Enter base hashtag: \")\n",
        "u_time = input(\"Enter time: \")\n",
        "u_location = input(\"Enter Location: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nvReo2xTQIG",
        "outputId": "d7ec1bb6-88b1-4d4d-d551-bea8e937c2c8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter base hashtag: hijab\n",
            "Enter time: 2022-02-19\n",
            "Enter Location: India\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing"
      ],
      "metadata": {
        "id": "bW6BjAZVUyuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "tweet_query = []\n",
        "format = '%Y-%m-%d'\n",
        "u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]\n",
        "print(df_query.shape[0])\n",
        "if df_query.shape[0]<50:\n",
        "  df_query = df.loc[df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]\n",
        "  df_query = df.iloc[:min(df_query.shape[0],1000),:]"
      ],
      "metadata": {
        "id": "Ih0NQpCHTVVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf95fd5d-df09-4c47-ff9c-1a67bb1655f6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_extractor(dataset):\n",
        "  preprocessed_vocabulary = dict()\n",
        "\n",
        "  #Converting to lowercase\n",
        "  def to_lower_case(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "  def remove_at_word(text):\n",
        "    data = text.split()\n",
        "    data = [d for d in data if d[0]!='@']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_hashtag(text):\n",
        "    data = text.split()\n",
        "    data = [d if (d[0]!='#' or len(d) == 1) else d[1:] for d in data]\n",
        "    data = [d for d in data if d[0]!='#']\n",
        "    text = ' '.join(data)\n",
        "    return text\n",
        "\n",
        "  def remove_URL(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r'bit.ly\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "  #Removing stopwords\n",
        "  def remove_stopwords(text):\n",
        "    stopword = stopwords.words('english')\n",
        "    new_list = [x for x in text.split() if x not in stopword]\n",
        "    return ' '.join(new_list)\n",
        "\n",
        "  #Removing punctuations\n",
        "  def remove_punctuations(text):\n",
        "    punctuations = '''!()-[|]`{};:'\"\\,<>./?@#$=+%^&*_~'''\n",
        "    new_list = ['' if x in punctuations else x for x in text.split()]\n",
        "    new_list_final = []\n",
        "    for token in new_list:\n",
        "      new_token=\"\"\n",
        "      for char in token:\n",
        "        if(char not in punctuations):\n",
        "          new_token+=char\n",
        "      if(len(new_token)!=0):\n",
        "        new_list_final.append(new_token)\n",
        "    return ' '.join(new_list_final)\n",
        "\n",
        "  #Tokenization\n",
        "  def tokenization(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "  def pre_process(text):\n",
        "    text = to_lower_case(text)\n",
        "    text = remove_at_word(text)\n",
        "    text = remove_hashtag(text)\n",
        "    text = remove_URL(text)\n",
        "    text = remove_stopwords(text)\n",
        "    text = remove_punctuations(text)\n",
        "    text = tokenization(text)\n",
        "    for token in text:\n",
        "      if token in preprocessed_vocabulary.keys():\n",
        "        preprocessed_vocabulary[token] += 1\n",
        "      else:\n",
        "        preprocessed_vocabulary[token] = 1\n",
        "    return text\n",
        "  \n",
        "  preprocessed_data = [pre_process(text) for text in dataset]\n",
        "\n",
        "  #print(preprocessed_vocabulary)\n",
        "\n",
        "  if len(preprocessed_vocabulary) > 0:\n",
        "    AOF_coefficient = sum(preprocessed_vocabulary.values())/len(preprocessed_vocabulary)\n",
        "  else:\n",
        "    AOF_coefficient = 0\n",
        "\n",
        "  vocabulary = {token.strip():preprocessed_vocabulary[token] for token in preprocessed_vocabulary.keys() if preprocessed_vocabulary[token] > AOF_coefficient and len(token.strip())}\n",
        "\n",
        "  #print(vocabulary)\n",
        "\n",
        "  final_tokens_per_tweet = []\n",
        "  for data in preprocessed_data:\n",
        "    final_tokens_per_tweet.append([token for token in data if token in vocabulary.keys()])\n",
        "\n",
        "  #print(preprocessed_data)\n",
        "  #print(final_tokens_per_tweet)\n",
        "\n",
        "  word2id = dict()\n",
        "  id2word = dict()\n",
        "  vocabulary_size = len(vocabulary)\n",
        "  count = 0\n",
        "  for token in vocabulary.keys():\n",
        "    word2id[token] = count\n",
        "    id2word[count] = token\n",
        "    count += 1\n",
        "\n",
        "  #print(word2id)\n",
        "  #print(id2word)\n",
        "\n",
        "  directed_graph_adjacency_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  edge_weight_matrix = np.zeros((vocabulary_size, vocabulary_size))\n",
        "  first_frequency = dict()\n",
        "  last_frequency = dict()\n",
        "  term_frequency = vocabulary\n",
        "  strength = dict()\n",
        "  degree = dict()\n",
        "  selective_centraility = dict()\n",
        "\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "    if len(tweet)<1:\n",
        "      continue\n",
        "\n",
        "    if tweet[0] in first_frequency.keys():\n",
        "      first_frequency[tweet[0]] += 1\n",
        "    else:\n",
        "      first_frequency[tweet[0]] = 1\n",
        "\n",
        "    if tweet[-1] in last_frequency.keys():\n",
        "      last_frequency[tweet[-1]] += 1\n",
        "    else:\n",
        "      last_frequency[tweet[-1]] = 1\n",
        "    \n",
        "\n",
        "\n",
        "    for i in range(len(tweet)-1):\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "      directed_graph_adjacency_matrix[x][y] += 1\n",
        "\n",
        "  for tweet in final_tokens_per_tweet:\n",
        "    for i in range(len(tweet)-1):\n",
        "\n",
        "\n",
        "      if tweet[i] == tweet[i+1]:\n",
        "        continue\n",
        "      x = word2id[tweet[i]]\n",
        "      y = word2id[tweet[i+1]]\n",
        "\n",
        "    # Updating degree..\n",
        "      if tweet[i] in degree.keys():\n",
        "        degree[tweet[i]] += 1\n",
        "      else:\n",
        "        degree[tweet[i]] = 1\n",
        "        \n",
        "      if tweet[i+1] in degree.keys():\n",
        "        degree[tweet[i+1]] += 1\n",
        "      else:\n",
        "        degree[tweet[i+1]] = 1\n",
        "\n",
        "      edge_weight_matrix[x][y] = directed_graph_adjacency_matrix[x][y]/(vocabulary[tweet[i]] + vocabulary[tweet[i+1]] - directed_graph_adjacency_matrix[x][y])\n",
        "\n",
        "      if tweet[i] in strength.keys():\n",
        "        strength[tweet[i]] += edge_weight_matrix[x][y]\n",
        "      else:\n",
        "        strength[tweet[i]] = edge_weight_matrix[x][y]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  first_frequency = {token:(first_frequency[token]/vocabulary[token] if token in first_frequency else 0) for token in vocabulary.keys()}\n",
        "  last_frequency = {token:(last_frequency[token]/vocabulary[token] if token in last_frequency else 0) for token in vocabulary.keys()}\n",
        "  degree = {token:(degree[token] if token in degree else 0) for token in vocabulary.keys()}\n",
        "  strength = {token:(strength[token] if token in strength else 0) for token in vocabulary.keys()}\n",
        "  selective_centraility = {token:(strength[token]/degree[token] if degree[token]!=0 else 0) for token in vocabulary.keys()}\n",
        "\n",
        "  #print(degree)\n",
        "  #print(vocabulary)\n",
        "\n",
        "  maxdegree = max(degree.items(), key=lambda x: x[1])[1]\n",
        "  max_degree_nodes_with_freq = {key:term_frequency[key] for key in degree.keys() if degree[key] == maxdegree}\n",
        "  maxfreq = max(max_degree_nodes_with_freq.items(), key=lambda x: x[1])[1]\n",
        "  central_node_name = [key for key in max_degree_nodes_with_freq.keys() if max_degree_nodes_with_freq[key] == maxfreq][0]\n",
        "  #print(\"central node: \", central_node_name)\n",
        "\n",
        "  # bfs\n",
        "  distance_from_central_node = dict()\n",
        "  central_node_id = word2id[central_node_name]\n",
        "  q = [(central_node_id, 0)]\n",
        "\n",
        "  # Set source as visited\n",
        "  distance_from_central_node[central_node_name] = 0\n",
        "\n",
        "  while q:\n",
        "      vis = q[0]\n",
        "      # Print current node\n",
        "      #print(id2word[vis[0]], vis[1])\n",
        "      q.pop(0)\n",
        "        \n",
        "      # For every adjacent vertex to\n",
        "      # the current vertex\n",
        "      for i in range(len(directed_graph_adjacency_matrix[vis[0]])):\n",
        "          if (directed_graph_adjacency_matrix[vis[0]][i] == 1 and (id2word[i] not in distance_from_central_node.keys())):\n",
        "              # Push the adjacent node\n",
        "              # in the queue\n",
        "              q.append((i, vis[1]+1))\n",
        "              distance_from_central_node[id2word[i]] = vis[1]+1\n",
        "\n",
        "  #print(distance_from_central_node)\n",
        "  inverse_distance_from_central_node = {token:(1/distance_from_central_node[token] if token in distance_from_central_node and token != central_node_name else 0) for token in vocabulary.keys()}\n",
        "  inverse_distance_from_central_node[central_node_name] = 1.0\n",
        "  #print(inverse_distance_from_central_node)\n",
        "\n",
        "  neighbour_importance = dict()\n",
        "\n",
        "  for i in range(len(directed_graph_adjacency_matrix)):\n",
        "    neighbours = set()\n",
        "\n",
        "    # traversing outgoing edges\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[i][j] > 0:\n",
        "        neighbours.add(j)\n",
        "    for j in range(len(directed_graph_adjacency_matrix)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          neighbours.add(j)\n",
        "    if len(neighbours) != 0:\n",
        "      neighbour_importance[id2word[i]] = sum([strength[id2word[j]] for j in neighbours])/len(neighbours)\n",
        "    else:\n",
        "      neighbour_importance[id2word[i]] = 0\n",
        "      \n",
        "  #print(neighbour_importance)\n",
        "\n",
        "  unnormalized_node_weight = {node: (first_frequency[node] + last_frequency[node] + term_frequency[node] + selective_centraility[node] + inverse_distance_from_central_node[node] + neighbour_importance[node]) for node in vocabulary.keys()}\n",
        "  max_node_weight = max(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  min_node_weight = min(unnormalized_node_weight.items(), key=lambda x: x[1])[1]\n",
        "  #print(\"max node weight: \", max_node_weight, \"min node weight: \", min_node_weight)\n",
        "  normalized_node_weight = {node: ((unnormalized_node_weight[node] - min_node_weight)/(max_node_weight - min_node_weight) if max_node_weight != min_node_weight else unnormalized_node_weight[node]) for node in unnormalized_node_weight.keys()}\n",
        "  #print(\"Unnormalized score: \", unnormalized_node_weight)\n",
        "  #print(\"Normalized score: \", normalized_node_weight)\n",
        "\n",
        "  damping_factor = 0.85\n",
        "  relevance_of_node = {node: np.random.uniform(0,1,1)[0] for node in vocabulary.keys()}\n",
        "  threshold = 0.000000001\n",
        "\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "\n",
        "  count = 0\n",
        "  while True:\n",
        "    count += 1\n",
        "    current_relevance_of_node = dict()\n",
        "    for node in vocabulary.keys():\n",
        "      outer_sum = 0\n",
        "      node_idx = word2id[node]\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if j == node_idx:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][node_idx] > 0:\n",
        "          den_sum = 0\n",
        "          for k in range(len(directed_graph_adjacency_matrix)):\n",
        "            if k == j:\n",
        "              continue\n",
        "            den_sum += directed_graph_adjacency_matrix[j][k]\n",
        "          outer_sum += ((directed_graph_adjacency_matrix[j][node_idx]/den_sum) * relevance_of_node[id2word[j]])\n",
        "      current_relevance_of_node[node] = (1-damping_factor)*normalized_node_weight[node] + damping_factor*normalized_node_weight[node]*outer_sum\n",
        "    \n",
        "\n",
        "    # checking convergence..\n",
        "    sq_error = sum([(current_relevance_of_node[node] - relevance_of_node[node])**2 for node in vocabulary.keys()])\n",
        "    relevance_of_node = current_relevance_of_node\n",
        "    if sq_error < threshold:\n",
        "      break\n",
        "\n",
        "  #print(relevance_of_node)\n",
        "  #print(count)\n",
        "\n",
        "  degree_centrality  = {node: 0 for node in vocabulary.keys()}\n",
        "\n",
        "  if len(directed_graph_adjacency_matrix) > 1:\n",
        "    for i in range(len(directed_graph_adjacency_matrix)):\n",
        "      count = 0\n",
        "      for j in range(len(directed_graph_adjacency_matrix)):\n",
        "        if i == j:\n",
        "          continue\n",
        "        if directed_graph_adjacency_matrix[j][i] > 0:\n",
        "          count += 1\n",
        "      degree_centrality[id2word[i]] = count / (len(directed_graph_adjacency_matrix)-1)\n",
        "\n",
        "  #print(degree_centrality)\n",
        "\n",
        "  final_keyword_rank = [{'node': node, 'NE_rank': relevance_of_node[node], 'Degree': degree_centrality[node]} for node in vocabulary.keys()]\n",
        "\n",
        "  #print(\"-----------\")\n",
        "  final_keyword_rank = sorted(final_keyword_rank, key = lambda i: (i['NE_rank'], i['Degree']), reverse = True)\n",
        "\n",
        "  final_keywords = [keyword['node'] for keyword in final_keyword_rank]\n",
        "\n",
        "  return final_keywords"
      ],
      "metadata": {
        "id": "FVkoAcGos6SQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in df_query['Preprocessed_Data']:\n",
        "  tweet_query.extend(tweet)"
      ],
      "metadata": {
        "id": "vgHmoH0ST3K0"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keyword_keywordextractor(df_query):\n",
        "  keyword_dataset = df_query['tweet'].tolist()\n",
        "  tweet_query_keyword_extractor = keyword_extractor(keyword_dataset)\n",
        "  return tweet_query_keyword_extractor"
      ],
      "metadata": {
        "id": "IhXLi1MqtCFt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_yake_keywords(tweet_query):\n",
        "  tweet_keywords_yake = []\n",
        "  kw_extractor = yake.KeywordExtractor(top=20, stopwords=None)\n",
        "  keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "  #keywords = kw_extractor.extract_keywords(' '.join(df_query['tweet'].tolist()))\n",
        "  for kw, v in keywords:\n",
        "    # print(\"Keyphrase: \",kw, \": score\", v)\n",
        "    for key in kw.split():\n",
        "      if(key.lower() not in tweet_keywords_yake):\n",
        "        tweet_keywords_yake.append(key.lower())\n",
        "  return tweet_keywords_yake"
      ],
      "metadata": {
        "id": "UZDm6Cznts-9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rake_keywords(tweet_query):\n",
        "  tweet_keywords_rake = []\n",
        "  rake = Rake()\n",
        "  rake_keywords = rake.apply(' '.join(tweet_query).encode('ascii', 'ignore').decode())\n",
        "  for kw,score in rake_keywords[:20]:\n",
        "    for key in kw.split():\n",
        "      if(key.lower() not in tweet_keywords_rake):\n",
        "        tweet_keywords_rake.append(key.lower())\n",
        "  # print(tweet_keywords_rake)\n",
        "  return tweet_keywords_rake"
      ],
      "metadata": {
        "id": "CikGUzazVjs-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_rank_keywords(tweet_query):\n",
        "  tweet_keywords_text_rank = []\n",
        "  TR_keywords = summa_keywords.keywords(' '.join(tweet_query), scores=True)\n",
        "  for kw,score in TR_keywords[:20]:\n",
        "    for key in kw.split():\n",
        "      if(key.lower() not in tweet_keywords_text_rank):\n",
        "        tweet_keywords_text_rank.append(key.lower())\n",
        "  # print(tweet_keywords_text_rank)\n",
        "  return tweet_keywords_text_rank"
      ],
      "metadata": {
        "id": "eFcU3G6BYv3I"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keybert_keywords(tweet_query):\n",
        "  # uncomment later\n",
        "  keybert_model = KeyBERT(model='all-mpnet-base-v2')\n",
        "  keybert_keywords = keybert_model.extract_keywords(' '.join(tweet_query), keyphrase_ngram_range=(1,1), stop_words='english', highlight=False, top_n=20)\n",
        "  tweet_keywords_keybert = list(dict(keybert_keywords).keys())\n",
        "  # print(tweet_keywords_keybert)\n",
        "  return tweet_keywords_keybert"
      ],
      "metadata": {
        "id": "x61foNdZaQId"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpful Functions"
      ],
      "metadata": {
        "id": "DsKQrbhRUptK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_preprocessed = []"
      ],
      "metadata": {
        "id": "0fZNQhToT43g"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing file name and data\n",
        "total_documents = 0\n",
        "path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "for filename in glob(os.path.join(path, '*')):\n",
        "   with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "     filename = os.path.basename(f.name)\n",
        "     data = json.load(f)\n",
        "     d_date = data[\"Date\"]\n",
        "     if(d_date==\"\" or d_date==\"Date\"):\n",
        "       continue\n",
        "     format = '%Y-%m-%d'\n",
        " \n",
        "     try:\n",
        "       d_present_date = datetime.datetime.strptime(d_date, format)\n",
        "     except:\n",
        "       continue\n",
        " \n",
        "     if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "       continue\n",
        "   \n",
        "     docs_preprocessed.append({'Name':filename, 'Data':data})\n",
        "     total_documents+=1\n",
        "print(total_documents)"
      ],
      "metadata": {
        "id": "I6fzO-H6T83w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f63b65-fd80-4948-c2c5-3893068cd855"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed):\n",
        "  relevant_docs_list = []\n",
        "  for doc in docs_preprocessed:\n",
        "    if doc['Data']['Base Hashtag']==base_hashtag:\n",
        "      current_date = datetime.datetime.strptime(base_date, format)\n",
        "      prev_date = current_date - datetime.timedelta(days=1)\n",
        "      next_date = current_date + datetime.timedelta(days=1)\n",
        "      #if(doc['Data']['Date'] in [str(prev_date.date()), str(current_date.date()), str(next_date.date())]):\n",
        "      #  relevant_docs_list.append((doc['Name'], doc['Data']['Location'].lower()))\n",
        "      if(doc['Data']['Date']==str(str(current_date.date()))):\n",
        "        relevant_docs_list.append((doc['Name'], doc['Data']['Location'].lower(), 1.0))\n",
        "      elif(doc['Data']['Date'] in [str(prev_date.date()), str(next_date.date())]):\n",
        "        relevant_docs_list.append((doc['Name'], doc['Data']['Location'].lower(), 0.5))\n",
        "  \n",
        "  # prioritize location\n",
        "  location_relevant = []\n",
        "  location_irrelevant = []\n",
        "  for x in relevant_docs_list:\n",
        "    if u_location.lower() in x[1]:\n",
        "      location_relevant.append(x)\n",
        "    else:\n",
        "      location_irrelevant.append(x)\n",
        "  relevant_docs_list = location_relevant + location_irrelevant\n",
        "  return relevant_docs_list"
      ],
      "metadata": {
        "id": "ASFbpl047xKd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nDCG(base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  ground_truth = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  ground_truth_scores = {}\n",
        "  ground_truth_scores_list = []\n",
        "  prediction_list_scores = []\n",
        "  for gt in ground_truth:\n",
        "    ground_truth_scores[gt[0]] = gt[2]\n",
        "    ground_truth_scores_list.append(gt[2])\n",
        "  for x in prediction_list:\n",
        "    if x[0] in ground_truth_scores.keys():\n",
        "      prediction_list_scores.append(ground_truth_scores[x[0]])\n",
        "    else:\n",
        "      prediction_list_scores.append(0.0)\n",
        "  \n",
        "  DCG = prediction_list_scores[0] + sum([prediction_list_scores[i]/np.log2(i+1) for i in range(1,len(prediction_list_scores))])\n",
        "  ideal_DCG = ground_truth_scores_list[0] + sum([ground_truth_scores_list[i]/np.log2(i+1) for i in range(1,len(ground_truth_scores_list))])\n",
        "  if ideal_DCG==0:\n",
        "    return DCG\n",
        "  return DCG/ideal_DCG"
      ],
      "metadata": {
        "id": "zYZY0MjUM8N5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  num_of_relevant_results=0\n",
        "  list_of_rel_doc_names = []\n",
        "  for x in relevant_docs_list:\n",
        "    list_of_rel_doc_names.append(x[0])\n",
        "  for itr in range(min(len(prediction_list), k)):\n",
        "    if (prediction_list[itr][0] in list_of_rel_doc_names):\n",
        "      num_of_relevant_results+=1\n",
        "  return num_of_relevant_results/k"
      ],
      "metadata": {
        "id": "JdWZkYhc7yh7"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_precision=0\n",
        "  ctr=0\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  # print(len(relevant_docs_list))\n",
        "  for k_val in range(1,len(relevant_docs_list)+1):\n",
        "    ctr+=1\n",
        "    if k_val>len(relevant_docs_list):\n",
        "      break\n",
        "    precision_at_k_val = precision_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Precision@{}: {}'.format(base_hashtag, k_val, precision_at_k_val))\n",
        "    average_precision += precision_at_k_val\n",
        "  return average_precision/ctr"
      ],
      "metadata": {
        "id": "k1XBPmJv73xD"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_at_k(k, base_hashtag, base_date, prediction_list, docs_preprocessed):\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  current_num_of_relevant_results=0\n",
        "  list_of_rel_doc_names = []\n",
        "  for x in relevant_docs_list:\n",
        "    list_of_rel_doc_names.append(x[0])\n",
        "  for itr in range(min(len(prediction_list),k)):\n",
        "    if (prediction_list[itr][0] in list_of_rel_doc_names):\n",
        "      current_num_of_relevant_results+=1\n",
        "  if(len(relevant_docs_list)==0):\n",
        "    return 0\n",
        "  return current_num_of_relevant_results/len(relevant_docs_list)"
      ],
      "metadata": {
        "id": "YJuX6kb277hg"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_recall(max_k, base_hashtag, base_date, relevant_docs, docs_preprocessed):\n",
        "  average_recall=0\n",
        "  ctr=0\n",
        "  relevant_docs_list = get_relevant_docs_list_for_base_hashtag(base_hashtag, base_date, docs_preprocessed)\n",
        "  # print(len(relevant_docs_list))\n",
        "  for k_val in range(1,len(relevant_docs_list)+1):\n",
        "    ctr+=1\n",
        "    if k_val>len(relevant_docs_list):\n",
        "      break\n",
        "    recall_at_k_val = recall_at_k(k_val, base_hashtag, base_date, relevant_docs, docs_preprocessed)\n",
        "    #print('Hashtag: {}   Recall@{}: {}'.format(base_hashtag, k_val, recall_at_k_val))\n",
        "    average_recall += recall_at_k_val\n",
        "  return average_recall/ctr"
      ],
      "metadata": {
        "id": "DiHZ2YSi783Y"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, processed_query):\n",
        "  max_list_size = len(get_relevant_docs_list_for_base_hashtag(u_base_hashtag, u_time, docs_preprocessed))\n",
        "  cosine_similarities_cv = {}\n",
        "  for document in docs_preprocessed:\n",
        "    query_sent = ' '.join(map(str, processed_query))\n",
        "    doc_text_sent = ' '.join(map(str, document['Data']['Body_processed']))\n",
        "    data = [query_sent, doc_text_sent]\n",
        "    count_vectorizer = CountVectorizer(encoding='latin-1', decode_error='ignore', ngram_range=(1,2))\n",
        "    vector_matrix = count_vectorizer.fit_transform(data)\n",
        "    cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
        "    cosine_similarities_cv[document['Name']] = cosine_similarity_matrix[0][1]\n",
        "  relevant_docs = list( sorted(cosine_similarities_cv.items(), key=operator.itemgetter(1),reverse=True))[:max_list_size]\n",
        "  for i in range(len(relevant_docs)):\n",
        "    for j in range(len(docs_preprocessed)):\n",
        "      if(relevant_docs[i][0] == docs_preprocessed[j]['Name']):\n",
        "        relevant_docs[i] = (relevant_docs[i][0], relevant_docs[i][1], docs_preprocessed[j]['Data']['Date'], docs_preprocessed[j]['Data']['Location'].lower(), docs_preprocessed[j]['Data']['Link'])\n",
        "\n",
        "  # prioritize location\n",
        "  location_relevant = []\n",
        "  location_irrelevant = []\n",
        "  for x in relevant_docs:\n",
        "    if u_location.lower() in x[3]:\n",
        "      location_relevant.append(x)\n",
        "    else:\n",
        "      location_irrelevant.append(x) \n",
        "  relevant_docs = location_relevant + location_irrelevant\n",
        "  return relevant_docs"
      ],
      "metadata": {
        "id": "kq1KVcjncYkG"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plain Model (Cosine Similarity Count Vectorizer)"
      ],
      "metadata": {
        "id": "Ty_NvU4DuLSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plain Model without YAKE / Keyword Extraction (Cosine Similarity Count Vectorizer)\n",
        "relevant_docs_cs_cv_plain = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_query)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_cs_cv_plain):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_cs_cv_plain = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_cs_cv_plain, docs_preprocessed)\n",
        "print('Mean Average Precision Plain Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_precision_hashtag_cs_cv_plain))\n",
        "\n",
        "mean_average_recall_hashtag_cs_cv_plain = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_cs_cv_plain, docs_preprocessed)\n",
        "print('Mean Average Recall Plain Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_recall_hashtag_cs_cv_plain))\n",
        "\n",
        "nDCG_hashtag_cs_cv_plain = nDCG(u_base_hashtag, u_time, relevant_docs_cs_cv_plain, docs_preprocessed)\n",
        "print('nDCG Plain Model (Cosine Similarity Count Vectorizer) : {}'.format(nDCG_hashtag_cs_cv_plain))"
      ],
      "metadata": {
        "id": "LsU5NnWRuYFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626ceca3-40e9-4dad-d956-043a674b1de2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_333.json', 0.473432260277448, '2022-02-20', 'kerala, india', 'https://www.theweek.in/news/entertainment/2022/02/20/hijab-not-a-choice-but-an-obligation-in-islam-says-zaira-wasim.html')\n",
            "Rank: 2 Relevant Document: ('hijab_340.json', 0.4510836597730487, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 3 Relevant Document: ('hijab_338.json', 0.4418573069201409, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 4 Relevant Document: ('hijab_285.json', 0.31066272422179314, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 5 Relevant Document: ('hijab_332.json', 0.3064880715700302, '2022-02-20', 'new delhi, india', 'https://thewire.in/communalism/karnataka-muslim-women-students-face-case-suspension-for-wearing-hijab-protesting')\n",
            "Rank: 6 Relevant Document: ('hijab_283.json', 0.2538146571452035, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 7 Relevant Document: ('hijab_337.json', 0.10191664989199703, '2022-02-20', 'new delhi, india', 'https://www.hindustantimes.com/opinion/hijab-row-restriction-on-clothing-is-political-101645367225790.html')\n",
            "Rank: 8 Relevant Document: ('hijab_336.json', 0.476639977757599, '2022-02-20', 'mumbai, maharashtra', 'https://www.indiatoday.in/movies/celebrities/story/hijab-isn-t-a-choice-but-an-obligation-in-islam-says-zaira-wasim-1915351-2022-02-20')\n",
            "Rank: 9 Relevant Document: ('hijab_339.json', 0.46668324772637026, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 10 Relevant Document: ('hijab_282.json', 0.4656657553919424, '2022-02-19', 'bangalore, karnataka', 'https://www.hindustantimes.com/entertainment/bollywood/zaira-wasim-reacts-to-hijab-row-i-as-a-woman-who-wears-hijab-resent-this-entire-system-101645288361058.html')\n",
            "Rank: 11 Relevant Document: ('hijab_288.json', 0.4219606821751756, '2022-02-19', 'mysore', 'https://thekashmirwalla.com/mysuru-college-cancels-uniform-order-allows-students-to-wear-hijab/')\n",
            "Rank: 12 Relevant Document: ('hijab_286.json', 0.3566226263019132, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 13 Relevant Document: ('hijab_334.json', 0.3474962808328208, '2022-02-20', 'massachusetts, united states of america', 'https://scroll.in/article/1017784/why-urbane-educated-muslim-women-are-wearing-the-hijab-in-india')\n",
            "Rank: 14 Relevant Document: ('hijab_281.json', 0.3379556349718695, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 15 Relevant Document: ('hijab_290.json', 0.3108300121917484, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 16 Relevant Document: ('hijab_287.json', 0.3035487172822035, '2022-02-19', 'shivamogga', 'https://www.outlookindia.com/national/karnataka-hijab-row-58-students-suspended-in-shivamogga-for-wearing-headscarves-holding-protest-news-183162')\n",
            "Rank: 17 Relevant Document: ('hijab_289.json', 0.3010499162111016, '2022-02-19', 'karnataka', 'https://www.freepressjournal.in/india/karnataka-hijab-row-seen-to-be-soft-cm-basavaraj-bommaiturns-heat-on-hijab-protesters')\n",
            "Rank: 18 Relevant Document: ('hijab_335.json', 0.277798164645885, '2022-02-20', 'bangalore, karnataka', 'https://indianexpress.com/article/cities/bangalore/karnataka-bengaluru-live-updates-hijab-row-protests-court-hearing-covid-7765244/')\n",
            "Rank: 19 Relevant Document: ('hijab_331.json', 0.2724327578578894, '2022-02-20', 'chennai, tamil nadu', 'https://www.thehindu.com/data/data-hijab-row-why-the-ban-is-a-double-blow-for-muslim-girl-students/article65066546.ece')\n",
            "Rank: 20 Relevant Document: ('hijab_284.json', 0.23542853108744544, '2022-02-19', 'bangalore, karnataka', 'https://www.sundayguardianlive.com/news/hijab-controversy-manufactured-create-anarchy')\n",
            "\n",
            "Mean Average Precision Plain Model (Cosine Similarity Count Vectorizer) : 1.0\n",
            "Mean Average Recall Plain Model (Cosine Similarity Count Vectorizer) : 0.5249999999999999\n",
            "nDCG Plain Model (Cosine Similarity Count Vectorizer) : 0.8922250479510986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with Keyword Extractor (Cosine Similarity Count Vectorizer)"
      ],
      "metadata": {
        "id": "v4sF3KZuvSuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with Keyword Extractor (Cosine Similarity Count Vectorizer)\n",
        "tweet_query_keyword_extractor = get_keyword_keywordextractor(df_query)\n",
        "relevant_docs_cs_cv_keyword_extractor = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_query_keyword_extractor[:20])\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_cs_cv_keyword_extractor):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_cs_cv_keyword_extractor = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_cs_cv_keyword_extractor, docs_preprocessed)\n",
        "print('Mean Average Precision Keyword Extractor Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_precision_hashtag_cs_cv_keyword_extractor))\n",
        "\n",
        "mean_average_recall_hashtag_cs_cv_keyword_extractor = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_cs_cv_keyword_extractor, docs_preprocessed)\n",
        "print('Mean Average Recall Keyword Extractor Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_recall_hashtag_cs_cv_keyword_extractor))\n",
        "\n",
        "nDCG_hashtag_cs_cv_keyword_extractor = nDCG(u_base_hashtag, u_time, relevant_docs_cs_cv_keyword_extractor, docs_preprocessed)\n",
        "print('nDCG Average Recall Keyword Extractor Model (Cosine Similarity Count Vectorizer) : {}'.format(nDCG_hashtag_cs_cv_keyword_extractor))"
      ],
      "metadata": {
        "id": "Y-CSUMHKvcI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f5561e-9818-4b75-adf4-ee9b7fd5ae87"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_338.json', 0.1774777717361945, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 2 Relevant Document: ('hijab_340.json', 0.15460649332904966, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 3 Relevant Document: ('hijab_333.json', 0.13950583874670525, '2022-02-20', 'kerala, india', 'https://www.theweek.in/news/entertainment/2022/02/20/hijab-not-a-choice-but-an-obligation-in-islam-says-zaira-wasim.html')\n",
            "Rank: 4 Relevant Document: ('hijab_285.json', 0.13300460536419248, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 5 Relevant Document: ('hijab_283.json', 0.11710382654898527, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 6 Relevant Document: ('hijab_332.json', 0.09464839094898682, '2022-02-20', 'new delhi, india', 'https://thewire.in/communalism/karnataka-muslim-women-students-face-case-suspension-for-wearing-hijab-protesting')\n",
            "Rank: 7 Relevant Document: ('hijab_339.json', 0.20307557176672805, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 8 Relevant Document: ('hijab_288.json', 0.20157932837411013, '2022-02-19', 'mysore', 'https://thekashmirwalla.com/mysuru-college-cancels-uniform-order-allows-students-to-wear-hijab/')\n",
            "Rank: 9 Relevant Document: ('hijab_331.json', 0.16554787810467414, '2022-02-20', 'chennai, tamil nadu', 'https://www.thehindu.com/data/data-hijab-row-why-the-ban-is-a-double-blow-for-muslim-girl-students/article65066546.ece')\n",
            "Rank: 10 Relevant Document: ('hijab_334.json', 0.1544636107197327, '2022-02-20', 'massachusetts, united states of america', 'https://scroll.in/article/1017784/why-urbane-educated-muslim-women-are-wearing-the-hijab-in-india')\n",
            "Rank: 11 Relevant Document: ('hijab_282.json', 0.15238194775913155, '2022-02-19', 'bangalore, karnataka', 'https://www.hindustantimes.com/entertainment/bollywood/zaira-wasim-reacts-to-hijab-row-i-as-a-woman-who-wears-hijab-resent-this-entire-system-101645288361058.html')\n",
            "Rank: 12 Relevant Document: ('hijab_336.json', 0.1466274317883391, '2022-02-20', 'mumbai, maharashtra', 'https://www.indiatoday.in/movies/celebrities/story/hijab-isn-t-a-choice-but-an-obligation-in-islam-says-zaira-wasim-1915351-2022-02-20')\n",
            "Rank: 13 Relevant Document: ('hijab_286.json', 0.13791217738859585, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 14 Relevant Document: ('hijab_281.json', 0.12292881858520574, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 15 Relevant Document: ('hijab_290.json', 0.11437725271791938, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 16 Relevant Document: ('hijab_284.json', 0.10405078918070554, '2022-02-19', 'bangalore, karnataka', 'https://www.sundayguardianlive.com/news/hijab-controversy-manufactured-create-anarchy')\n",
            "Rank: 17 Relevant Document: ('hijab_335.json', 0.09922778767136677, '2022-02-20', 'bangalore, karnataka', 'https://indianexpress.com/article/cities/bangalore/karnataka-bengaluru-live-updates-hijab-row-protests-court-hearing-covid-7765244/')\n",
            "Rank: 18 Relevant Document: ('hijab_287.json', 0.09912870847774079, '2022-02-19', 'shivamogga', 'https://www.outlookindia.com/national/karnataka-hijab-row-58-students-suspended-in-shivamogga-for-wearing-headscarves-holding-protest-news-183162')\n",
            "Rank: 19 Relevant Document: ('IPL_322.json', 0.06723991272665174, '2022-02-20', 'mumbai, maharashtra', 'https://www.livemint.com/companies/news/amazon-reliance-set-to-lock-horns-over-india-s-cricket-media-rights-11645353624943.html')\n",
            "Rank: 20 Relevant Document: ('hijab_289.json', 0.0630436117122604, '2022-02-19', 'karnataka', 'https://www.freepressjournal.in/india/karnataka-hijab-row-seen-to-be-soft-cm-basavaraj-bommaiturns-heat-on-hijab-protesters')\n",
            "\n",
            "Mean Average Precision Keyword Extractor Model (Cosine Similarity Count Vectorizer) : 0.9948684210526315\n",
            "Mean Average Recall Keyword Extractor Model (Cosine Similarity Count Vectorizer) : 0.5199999999999999\n",
            "nDCG Average Recall Keyword Extractor Model (Cosine Similarity Count Vectorizer) : 0.8779647375923737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with YAKE (Cosine Similarity Count Vectorizer)"
      ],
      "metadata": {
        "id": "IK7PnL_lwBwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with YAKE (Cosine Similarity Count Vectorizer)\n",
        "tweet_keywords_yake = get_yake_keywords(tweet_query)\n",
        "relevant_docs_cs_cv_yake = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_keywords_yake)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_cs_cv_yake):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_cs_cv_yake = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_cs_cv_yake, docs_preprocessed)\n",
        "print('Mean Average Precision YAKE Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_precision_hashtag_cs_cv_yake))\n",
        "\n",
        "mean_average_recall_hashtag_cs_cv_yake = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_cs_cv_yake, docs_preprocessed)\n",
        "print('Mean Average Recall YAKE Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_recall_hashtag_cs_cv_yake))\n",
        "\n",
        "nDCG_hashtag_cs_cv_yake = nDCG(u_base_hashtag, u_time, relevant_docs_cs_cv_yake, docs_preprocessed)\n",
        "print('nDCG Average Recall YAKE Model (Cosine Similarity Count Vectorizer) : {}'.format(nDCG_hashtag_cs_cv_yake))"
      ],
      "metadata": {
        "id": "OR41bEZawSfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547eba88-131c-48a5-86b4-08158faf1f71"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_338.json', 0.1769370022801819, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 2 Relevant Document: ('hijab_340.json', 0.16320220091309284, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 3 Relevant Document: ('hijab_333.json', 0.13989889080189652, '2022-02-20', 'kerala, india', 'https://www.theweek.in/news/entertainment/2022/02/20/hijab-not-a-choice-but-an-obligation-in-islam-says-zaira-wasim.html')\n",
            "Rank: 4 Relevant Document: ('hijab_283.json', 0.11846388255927363, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 5 Relevant Document: ('hijab_285.json', 0.11453627573905731, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 6 Relevant Document: ('hijab_332.json', 0.09688299479199727, '2022-02-20', 'new delhi, india', 'https://thewire.in/communalism/karnataka-muslim-women-students-face-case-suspension-for-wearing-hijab-protesting')\n",
            "Rank: 7 Relevant Document: ('hijab_339.json', 0.19229893912234056, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 8 Relevant Document: ('hijab_288.json', 0.1823885142625773, '2022-02-19', 'mysore', 'https://thekashmirwalla.com/mysuru-college-cancels-uniform-order-allows-students-to-wear-hijab/')\n",
            "Rank: 9 Relevant Document: ('hijab_334.json', 0.17186496221888803, '2022-02-20', 'massachusetts, united states of america', 'https://scroll.in/article/1017784/why-urbane-educated-muslim-women-are-wearing-the-hijab-in-india')\n",
            "Rank: 10 Relevant Document: ('hijab_282.json', 0.1489388671836248, '2022-02-19', 'bangalore, karnataka', 'https://www.hindustantimes.com/entertainment/bollywood/zaira-wasim-reacts-to-hijab-row-i-as-a-woman-who-wears-hijab-resent-this-entire-system-101645288361058.html')\n",
            "Rank: 11 Relevant Document: ('hijab_336.json', 0.14410507489585234, '2022-02-20', 'mumbai, maharashtra', 'https://www.indiatoday.in/movies/celebrities/story/hijab-isn-t-a-choice-but-an-obligation-in-islam-says-zaira-wasim-1915351-2022-02-20')\n",
            "Rank: 12 Relevant Document: ('hijab_286.json', 0.12010327366116524, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 13 Relevant Document: ('hijab_331.json', 0.11956708771050652, '2022-02-20', 'chennai, tamil nadu', 'https://www.thehindu.com/data/data-hijab-row-why-the-ban-is-a-double-blow-for-muslim-girl-students/article65066546.ece')\n",
            "Rank: 14 Relevant Document: ('hijab_281.json', 0.11633953918257944, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 15 Relevant Document: ('hijab_335.json', 0.10474458731327634, '2022-02-20', 'bangalore, karnataka', 'https://indianexpress.com/article/cities/bangalore/karnataka-bengaluru-live-updates-hijab-row-protests-court-hearing-covid-7765244/')\n",
            "Rank: 16 Relevant Document: ('hijab_287.json', 0.10027999959888001, '2022-02-19', 'shivamogga', 'https://www.outlookindia.com/national/karnataka-hijab-row-58-students-suspended-in-shivamogga-for-wearing-headscarves-holding-protest-news-183162')\n",
            "Rank: 17 Relevant Document: ('hijab_284.json', 0.09293792945374979, '2022-02-19', 'bangalore, karnataka', 'https://www.sundayguardianlive.com/news/hijab-controversy-manufactured-create-anarchy')\n",
            "Rank: 18 Relevant Document: ('hijab_290.json', 0.09175960479909608, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 19 Relevant Document: ('narendramodi_251.json', 0.062054210027785946, '2022-02-19', 'mumbai, maharashtra', 'https://timesofindia.indiatimes.com/sports/more-sports/others/pm-narendra-modi-hails-iocs-decision-to-award-india-hosting-rights-of-its-2023-session/articleshow/89692190.cms')\n",
            "Rank: 20 Relevant Document: ('hijab_289.json', 0.06100294560590712, '2022-02-19', 'karnataka', 'https://www.freepressjournal.in/india/karnataka-hijab-row-seen-to-be-soft-cm-basavaraj-bommaiturns-heat-on-hijab-protesters')\n",
            "\n",
            "Mean Average Precision YAKE Model (Cosine Similarity Count Vectorizer) : 0.9948684210526315\n",
            "Mean Average Recall YAKE Model (Cosine Similarity Count Vectorizer) : 0.5199999999999999\n",
            "nDCG Average Recall YAKE Model (Cosine Similarity Count Vectorizer) : 0.8787372651693676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine Similarity TF-IDF Vectorizer**"
      ],
      "metadata": {
        "id": "rPwKCN2vyROm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with RAKE (Cosine Similarity Count Vectorizer)"
      ],
      "metadata": {
        "id": "jQ-zx_PMVMBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with RAKE (Cosine Similarity Count Vectorizer)\n",
        "tweet_keywords_rake = get_rake_keywords(tweet_query)\n",
        "relevant_docs_cs_cv_rake = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_keywords_rake)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_cs_cv_rake):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_cs_cv_rake = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_cs_cv_rake, docs_preprocessed)\n",
        "print('Mean Average Precision RAKE Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_precision_hashtag_cs_cv_rake))\n",
        "\n",
        "mean_average_recall_hashtag_cs_cv_rake = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_cs_cv_rake, docs_preprocessed)\n",
        "print('Mean Average Recall RAKE Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_recall_hashtag_cs_cv_rake))\n",
        "\n",
        "nDCG_hashtag_cs_cv_rake = nDCG(u_base_hashtag, u_time, relevant_docs_cs_cv_rake, docs_preprocessed)\n",
        "print('nDCG Average Recall RAKE Model (Cosine Similarity Count Vectorizer) : {}'.format(nDCG_hashtag_cs_cv_rake))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx6OBhbuVRm1",
        "outputId": "f8bf1d4b-1492-42fd-9f53-b3f284280df2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_285.json', 0.0707278569701757, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 2 Relevant Document: ('hijab_338.json', 0.05692602182614826, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 3 Relevant Document: ('hijab_283.json', 0.050707444335893914, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 4 Relevant Document: ('kejriwalvsall_1655.json', 0.04849193006394983, '2022-02-18', 'india', 'https://www.ndtv.com/opinion/it-could-be-kejriwal-vs-adityanath-in-2029-by-sagarika-ghose-2814030')\n",
            "Rank: 5 Relevant Document: ('PunjabElections2022_2360.json', 0.045011789814932174, '2022-02-20', 'punjab india', 'https://indianexpress.com/elections/assembly-elections-2022-live-updates-punjab-up-uttarakhand-manipur-goa-7783132/')\n",
            "Rank: 6 Relevant Document: ('hijab_340.json', 0.04463105027099544, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 7 Relevant Document: ('narendramodi_205.json', 0.04042557675089088, '2022-02-18', 'new delhi, india', 'https://theprint.in/india/mansukh-mandaviya-to-release-book-on-pm-modis-leadership-in-indias-fight-against-covid-19/836097/')\n",
            "Rank: 8 Relevant Document: ('kejriwalvsall_1661.json', 0.04036439054324473, '2022-02-18', 'india', 'https://www.hindustantimes.com/cities/delhi-news/is-kejriwal-for-municipal-reforms-or-not-bjp-asks-as-unification-of-mcds-imminent-101646853880629.html')\n",
            "Rank: 9 Relevant Document: ('hijab_286.json', 0.0855954051953207, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 10 Relevant Document: ('EFFvsAfriforum_1652.json', 0.07085385901593472, '2022-02-18', 'south africa', 'https://tdpelmedia.com/african-literature-scholar-professor-elizabeth-gunner-to-testify-in-malema-hate-speech-case')\n",
            "Rank: 11 Relevant Document: ('EFFvsAfriforum_1653.json', 0.06776741475716971, '2022-02-18', 'south africa', 'https://ewn.co.za/2022/02/18/watch-live-malema-back-in-court-in-afriforum-hate-speech-case')\n",
            "Rank: 12 Relevant Document: ('BJPwinningUP_2527.json', 0.05460324724246339, '2022-02-18', 'lucknow, uttar pradesh', 'https://www.ndtv.com/india-news/up-assembly-election-second-hate-speech-video-by-bjp-mla-emerges-amid-uttar-pradesh-election-2775119')\n",
            "Rank: 13 Relevant Document: ('BJPwinningUP_2524.json', 0.05352309301050474, '2022-02-18', 'banaras, up', 'https://theprint.in/opinion/bjps-2ab-factor-thats-missing-from-sp-bsp-congress-in-up-elections/836455/')\n",
            "Rank: 14 Relevant Document: ('hijab_290.json', 0.05282859011668848, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 15 Relevant Document: ('hijab_339.json', 0.05000185801081426, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 16 Relevant Document: ('narendramodi_305.json', 0.0456535561083234, '2022-02-20', 'hardoi, uttar pradesh', 'https://www.indiatoday.in/elections/uttar-pradesh-assembly-polls-2022/story/pm-modi-addresses-rally-up-hardoi-1915489-2022-02-20')\n",
            "Rank: 17 Relevant Document: ('narendramodi_304.json', 0.044659670368041055, '2022-02-20', 'hardoi, uttar pradesh', 'https://indianexpress.com/elections/pm-narendra-modi-uttar-pradesh-polls-hardoi-7782343/')\n",
            "Rank: 18 Relevant Document: ('QueenElizabeth_2388.json', 0.04178799328883707, '2022-02-20', 'united kingdom', 'https://www.republicworld.com/world-news/uk-news/uk-prince-andrew-met-mother-queen-elizabeth-ii-every-night-last-week-claims-report-articleshow.html')\n",
            "Rank: 19 Relevant Document: ('narendramodi_310.json', 0.039774953569076864, '2022-02-20', 'amritsar, punjab', 'https://www.aljazeera.com/news/2022/2/20/farmer-anger-will-test-modi-as-indias-grain-bowl-votes')\n",
            "Rank: 20 Relevant Document: ('stormfranklin_2383.json', 0.03925595396451744, '2022-02-20', 'united kingdom', 'https://www.independent.co.uk/life-style/storm-franklin-flood-damage-home-b2019581.html')\n",
            "\n",
            "Mean Average Precision RAKE Model (Cosine Similarity Count Vectorizer) : 0.5625645521059608\n",
            "Mean Average Recall RAKE Model (Cosine Similarity Count Vectorizer) : 0.2425\n",
            "nDCG Average Recall RAKE Model (Cosine Similarity Count Vectorizer) : 0.500103904705956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with TextRank (Cosine Similarity Count Vectorizer)"
      ],
      "metadata": {
        "id": "rg9trzhdY5c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with TextRank (Cosine Similarity Count Vectorizer)\n",
        "tweet_keywords_text_rank = get_text_rank_keywords(tweet_query)\n",
        "relevant_docs_cs_cv_text_rank = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_keywords_text_rank)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_cs_cv_text_rank):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_cs_cv_text_rank = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_cs_cv_text_rank, docs_preprocessed)\n",
        "print('Mean Average Precision TextRank Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_precision_hashtag_cs_cv_text_rank))\n",
        "\n",
        "mean_average_recall_hashtag_cs_cv_text_rank = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_cs_cv_text_rank, docs_preprocessed)\n",
        "print('Mean Average Recall TextRank Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_recall_hashtag_cs_cv_text_rank))\n",
        "\n",
        "nDCG_hashtag_cs_cv_text_rank = nDCG(u_base_hashtag, u_time, relevant_docs_cs_cv_text_rank, docs_preprocessed)\n",
        "print('nDCG Average Recall TextRank Model (Cosine Similarity Count Vectorizer) : {}'.format(nDCG_hashtag_cs_cv_text_rank))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyBbW7xAY_um",
        "outputId": "d6825f9f-1afc-4c54-f7f8-da709d958fa1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_338.json', 0.14429466197283017, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 2 Relevant Document: ('hijab_340.json', 0.12121036539593946, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 3 Relevant Document: ('hijab_283.json', 0.09917590020751647, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 4 Relevant Document: ('hijab_332.json', 0.08861131664918266, '2022-02-20', 'new delhi, india', 'https://thewire.in/communalism/karnataka-muslim-women-students-face-case-suspension-for-wearing-hijab-protesting')\n",
            "Rank: 5 Relevant Document: ('hijab_285.json', 0.08537105231519927, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 6 Relevant Document: ('hijab_333.json', 0.06805341643047029, '2022-02-20', 'kerala, india', 'https://www.theweek.in/news/entertainment/2022/02/20/hijab-not-a-choice-but-an-obligation-in-islam-says-zaira-wasim.html')\n",
            "Rank: 7 Relevant Document: ('hijab_339.json', 0.13839803543487486, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 8 Relevant Document: ('hijab_334.json', 0.12558347041361478, '2022-02-20', 'massachusetts, united states of america', 'https://scroll.in/article/1017784/why-urbane-educated-muslim-women-are-wearing-the-hijab-in-india')\n",
            "Rank: 9 Relevant Document: ('hijab_284.json', 0.12364083370651363, '2022-02-19', 'bangalore, karnataka', 'https://www.sundayguardianlive.com/news/hijab-controversy-manufactured-create-anarchy')\n",
            "Rank: 10 Relevant Document: ('hijab_331.json', 0.10980143437715552, '2022-02-20', 'chennai, tamil nadu', 'https://www.thehindu.com/data/data-hijab-row-why-the-ban-is-a-double-blow-for-muslim-girl-students/article65066546.ece')\n",
            "Rank: 11 Relevant Document: ('hijab_288.json', 0.09833396294276475, '2022-02-19', 'mysore', 'https://thekashmirwalla.com/mysuru-college-cancels-uniform-order-allows-students-to-wear-hijab/')\n",
            "Rank: 12 Relevant Document: ('hijab_281.json', 0.08960564196092344, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 13 Relevant Document: ('hijab_282.json', 0.08718256754775804, '2022-02-19', 'bangalore, karnataka', 'https://www.hindustantimes.com/entertainment/bollywood/zaira-wasim-reacts-to-hijab-row-i-as-a-woman-who-wears-hijab-resent-this-entire-system-101645288361058.html')\n",
            "Rank: 14 Relevant Document: ('hijab_286.json', 0.08689816695494773, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 15 Relevant Document: ('hijab_287.json', 0.0805945631482241, '2022-02-19', 'shivamogga', 'https://www.outlookindia.com/national/karnataka-hijab-row-58-students-suspended-in-shivamogga-for-wearing-headscarves-holding-protest-news-183162')\n",
            "Rank: 16 Relevant Document: ('hijab_290.json', 0.06695429707585229, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 17 Relevant Document: ('hijab_336.json', 0.06577237289120895, '2022-02-20', 'mumbai, maharashtra', 'https://www.indiatoday.in/movies/celebrities/story/hijab-isn-t-a-choice-but-an-obligation-in-islam-says-zaira-wasim-1915351-2022-02-20')\n",
            "Rank: 18 Relevant Document: ('hijab_289.json', 0.0555276749981554, '2022-02-19', 'karnataka', 'https://www.freepressjournal.in/india/karnataka-hijab-row-seen-to-be-soft-cm-basavaraj-bommaiturns-heat-on-hijab-protesters')\n",
            "Rank: 19 Relevant Document: ('IPL_322.json', 0.05466803185018877, '2022-02-20', 'mumbai, maharashtra', 'https://www.livemint.com/companies/news/amazon-reliance-set-to-lock-horns-over-india-s-cricket-media-rights-11645353624943.html')\n",
            "Rank: 20 Relevant Document: ('BJPwinningUP_2527.json', 0.04613565027450549, '2022-02-18', 'lucknow, uttar pradesh', 'https://www.ndtv.com/india-news/up-assembly-election-second-hate-speech-video-by-bjp-mla-emerges-amid-uttar-pradesh-election-2775119')\n",
            "\n",
            "Mean Average Precision TextRank Model (Cosine Similarity Count Vectorizer) : 0.9923684210526315\n",
            "Mean Average Recall TextRank Model (Cosine Similarity Count Vectorizer) : 0.5175\n",
            "nDCG Average Recall TextRank Model (Cosine Similarity Count Vectorizer) : 0.8721267517825971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model with KeyBERT (Cosine Similarity Count Vectorizer)"
      ],
      "metadata": {
        "id": "YFQIDpqvaTYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with KeyBERT (Cosine Similarity Count Vectorizer)\n",
        "tweet_keywords_keybert = get_keybert_keywords(tweet_query)\n",
        "relevant_docs_cs_cv_keybert = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_keywords_keybert)\n",
        "\n",
        "for rank, doc in enumerate(relevant_docs_cs_cv_keybert):\n",
        "  print('Rank: {} Relevant Document: {}'.format(rank+1,doc))\n",
        "\n",
        "print()\n",
        "\n",
        "mean_average_precision_hashtag_cs_cv_keybert = mean_average_precision(20, u_base_hashtag, u_time, relevant_docs_cs_cv_keybert, docs_preprocessed)\n",
        "print('Mean Average Precision KeyBERT Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_precision_hashtag_cs_cv_keybert))\n",
        "\n",
        "mean_average_recall_hashtag_cs_cv_keybert = mean_average_recall(20, u_base_hashtag, u_time, relevant_docs_cs_cv_keybert, docs_preprocessed)\n",
        "print('Mean Average Recall KeyBERT Model (Cosine Similarity Count Vectorizer) : {}'.format(mean_average_recall_hashtag_cs_cv_keybert))\n",
        "\n",
        "nDCG_hashtag_cs_cv_keybert = nDCG(u_base_hashtag, u_time, relevant_docs_cs_cv_keybert, docs_preprocessed)\n",
        "print('nDCG Average Recall KeyBERT Model (Cosine Similarity Count Vectorizer) : {}'.format(nDCG_hashtag_cs_cv_keybert))"
      ],
      "metadata": {
        "id": "vXw3z3CLaY-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0f7c9da-4f73-44b4-e431-b674d8e1010f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1 Relevant Document: ('hijab_333.json', 0.06975291937335264, '2022-02-20', 'kerala, india', 'https://www.theweek.in/news/entertainment/2022/02/20/hijab-not-a-choice-but-an-obligation-in-islam-says-zaira-wasim.html')\n",
            "Rank: 2 Relevant Document: ('hijab_340.json', 0.060738265236412355, '2022-02-20', 'hyderabad, india', 'https://www.siasat.com/hijab-row-bangladesh-stands-in-solidarity-with-muslim-women-2279139/')\n",
            "Rank: 3 Relevant Document: ('hijab_338.json', 0.059159257245398174, '2022-02-20', 'new delhi, india', 'https://news.abplive.com/karnataka/hijab-row-conspiracy-of-bjp-against-muslim-girls-former-k-taka-cm-siddaramaiah-1514360')\n",
            "Rank: 4 Relevant Document: ('hijab_332.json', 0.04015386282684289, '2022-02-20', 'new delhi, india', 'https://thewire.in/communalism/karnataka-muslim-women-students-face-case-suspension-for-wearing-hijab-protesting')\n",
            "Rank: 5 Relevant Document: ('hijab_285.json', 0.03850133313173993, '2022-02-19', 'new delhi, india', 'https://news.abplive.com/karnataka/karnataka-hijab-row-govt-order-dated-feb-5-does-not-ban-hijab-ag-tells-high-court-1514213')\n",
            "Rank: 6 Relevant Document: ('hijab_283.json', 0.024396630531038598, '2022-02-19', 'new delhi, india', 'https://www.newindianexpress.com/nation/2022/feb/19/education-more-important-than-hijab-rss-linked-muslim-rashtriya-manch-to-community-2421449.html')\n",
            "Rank: 7 Relevant Document: ('hijab_337.json', 0.011647609296910203, '2022-02-20', 'new delhi, india', 'https://www.hindustantimes.com/opinion/hijab-row-restriction-on-clothing-is-political-101645367225790.html')\n",
            "Rank: 8 Relevant Document: ('hijab_336.json', 0.07078565672540507, '2022-02-20', 'mumbai, maharashtra', 'https://www.indiatoday.in/movies/celebrities/story/hijab-isn-t-a-choice-but-an-obligation-in-islam-says-zaira-wasim-1915351-2022-02-20')\n",
            "Rank: 9 Relevant Document: ('hijab_282.json', 0.06772531011516958, '2022-02-19', 'bangalore, karnataka', 'https://www.hindustantimes.com/entertainment/bollywood/zaira-wasim-reacts-to-hijab-row-i-as-a-woman-who-wears-hijab-resent-this-entire-system-101645288361058.html')\n",
            "Rank: 10 Relevant Document: ('hijab_339.json', 0.059728109343155304, '2022-02-20', 'aluva, kerala', 'https://thefederal.com/states/south/tamil-nadu/discrimination-violence-drove-40-dalits-in-tn-village-to-embrace-islam/')\n",
            "Rank: 11 Relevant Document: ('hijab_288.json', 0.05759409382117432, '2022-02-19', 'mysore', 'https://thekashmirwalla.com/mysuru-college-cancels-uniform-order-allows-students-to-wear-hijab/')\n",
            "Rank: 12 Relevant Document: ('hijab_289.json', 0.0472827087841953, '2022-02-19', 'karnataka', 'https://www.freepressjournal.in/india/karnataka-hijab-row-seen-to-be-soft-cm-basavaraj-bommaiturns-heat-on-hijab-protesters')\n",
            "Rank: 13 Relevant Document: ('hijab_281.json', 0.04662817256680218, '2022-02-19', 'bangalore, karnataka', 'https://www.thehindu.com/news/national/karnataka/hijab-row-polarises-classrooms-breaks-friendships/article65063188.ece')\n",
            "Rank: 14 Relevant Document: ('hijab_286.json', 0.044821457651293646, '2022-02-19', 'bangalore, karnataka', 'https://thefederal.com/states/south/karnataka/uneasy-bjp-central-command-wants-to-wind-down-hijab-row-in-karnataka/')\n",
            "Rank: 15 Relevant Document: ('hijab_335.json', 0.041344911529736156, '2022-02-20', 'bangalore, karnataka', 'https://indianexpress.com/article/cities/bangalore/karnataka-bengaluru-live-updates-hijab-row-protests-court-hearing-covid-7765244/')\n",
            "Rank: 16 Relevant Document: ('hijab_287.json', 0.041303628532391994, '2022-02-19', 'shivamogga', 'https://www.outlookindia.com/national/karnataka-hijab-row-58-students-suspended-in-shivamogga-for-wearing-headscarves-holding-protest-news-183162')\n",
            "Rank: 17 Relevant Document: ('hijab_290.json', 0.04117581097845097, '2022-02-19', 'bangalore, karnataka', 'https://english.varthabharati.in/karnataka/hijab-row-feb-5-government-order-doesnt-ban-hijab-insists-ag-in-karnataka-hc')\n",
            "Rank: 18 Relevant Document: ('hijab_334.json', 0.03757222963452958, '2022-02-20', 'massachusetts, united states of america', 'https://scroll.in/article/1017784/why-urbane-educated-muslim-women-are-wearing-the-hijab-in-india')\n",
            "Rank: 19 Relevant Document: ('hijab_331.json', 0.030495661756124188, '2022-02-20', 'chennai, tamil nadu', 'https://www.thehindu.com/data/data-hijab-row-why-the-ban-is-a-double-blow-for-muslim-girl-students/article65066546.ece')\n",
            "Rank: 20 Relevant Document: ('hijab_284.json', 0.024011720580162817, '2022-02-19', 'bangalore, karnataka', 'https://www.sundayguardianlive.com/news/hijab-controversy-manufactured-create-anarchy')\n",
            "\n",
            "Mean Average Precision KeyBERT Model (Cosine Similarity Count Vectorizer) : 1.0\n",
            "Mean Average Recall KeyBERT Model (Cosine Similarity Count Vectorizer) : 0.5249999999999999\n",
            "nDCG Average Recall KeyBERT Model (Cosine Similarity Count Vectorizer) : 0.8888743182235169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Across all hashtags"
      ],
      "metadata": {
        "id": "zxyJ3ooRzXwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_list =[['IPL', '2022-02-25', 'india'], ['IPL', '2022-02-16', 'india'], ['Drishyam2', '2022-02-17', 'india'], ['elections', '2022-02-27', 'imphal'], ['elections', '2022-02-27', 'new delhi'], ['elections', '2022-02-27', 'uttar pradesh'], ['nursesstrike', '2022-02-15', 'australia'], ['Djokovic', '2022-02-15', 'australia'],['hijab', '2022-02-19', 'india'], ['msdtrong', '2022-02-14', 'Florida'], ['ottawaoccupied', '2022-02-14', 'Ottawa'], ['Boycott_ChennaiSuperKings', '2022-02-14', 'chennai'], ['GlanceJio', '2022-02-14', 'New Delhi'], ['ArabicKuthu', '2022-02-14', 'CHENNAI'], ['Djokovic', '2022-02-15', 'AUSTRALIA'], ['Real Madrid', '2022-02-15', 'Madrid'], ['bighit', '2022-02-15', 'ASSAM'], ['Maxwell', '2022-02-15', 'india'], ['mafsau', '2022-02-16', 'australia'], ['channi', '2022-02-16', 'punjab'], ['ayalaan', '2022-02-16', 'tamil nadu'], ['jkbose', '2022-02-16', 'srinagar'], ['happybirthdayjhope', '2022-02-17', 'new delhi'], ['mohsinbaig', '2022-02-17', 'islamabad'],  ['ShivajiJayanti', '2022-02-18', 'maharashtra'], ['OperationDudula', '2022-02-20', 'South Africa'], ['UFCVegas48', '2022-02-20', 'india'], ['FCNPSG', '2022-02-20', 'PARIS'], ['shivamogga', '2022-02-21', 'KARNATAKA'],  ['StayAlive_CHAKHO', '2022-02-22', 'seoul'], ['KaranSinghGrover', '2022-02-22', 'india'], ['NationalMargaritaDay', '2022-02-22', 'United States of America'], ['dontsaygay', '2022-02-22', 'New York'], ['RIPRikyRick', '2022-02-23', 'South Africa'], ['budget2022', '2022-02-23', 'Cape Town'], ['CottonFest2022', '2022-02-23', 'nigeria'], ['NationalChiliDay', '2022-02-24', 'Ohio'], ['stockmarketcrash', '2022-02-24', 'mumbai'], ['BidenisaFailure', '2022-02-24', 'United States of America']]\n",
        "print(len(global_list))\n",
        "global_list = global_list[:30]"
      ],
      "metadata": {
        "id": "Ft7ith1b86zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff359494-076d-46f5-e962-64d4f129cb47"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_average_mean_average_precision_plain = []\n",
        "global_mean_average_recall_plain = []\n",
        "global_mean_average_ndcg_plain = []\n",
        "\n",
        "global_average_mean_average_precision_keyword_extractor = []\n",
        "global_mean_average_recall_keyword_extractor = []\n",
        "global_mean_average_ndcg_keyword_extractor = []\n",
        "\n",
        "global_average_mean_average_precision_rake = []\n",
        "global_mean_average_recall_rake = []\n",
        "global_mean_average_ndcg_rake = []\n",
        "\n",
        "global_average_mean_average_precision_yake = []\n",
        "global_mean_average_recall_yake = []\n",
        "global_mean_average_ndcg_yake = []\n",
        "\n",
        "global_average_mean_average_precision_text_rank = []\n",
        "global_mean_average_recall_text_rank = []\n",
        "global_mean_average_ndcg_text_rank = []\n",
        "\n",
        "global_average_mean_average_precision_keybert = []\n",
        "global_mean_average_recall_keybert = []\n",
        "global_mean_average_ndcg_keybert = []\n",
        "\n",
        "for iter in tqdm(range(len(global_list))):\n",
        "  u_base_hashtag = global_list[iter][0]\n",
        "  u_time = global_list[iter][1]\n",
        "  u_location = global_list[iter][2]\n",
        "  tweet_query = []\n",
        "  format = '%Y-%m-%d'\n",
        "  u_present_date = datetime.datetime.strptime(u_time, format)\n",
        "  u_prev_date = u_present_date - datetime.timedelta(days=1)\n",
        "  u_next_date = u_present_date + datetime.timedelta(days=1)\n",
        "  df_query = df.loc[df['hashtags'].str.contains(u_base_hashtag) & df['Date_Only'].isin([str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())])]\n",
        "\n",
        "  for tweet in df_query['Preprocessed_Data']:\n",
        "    tweet_query.extend(tweet)\n",
        "  \n",
        "  tweet_keywords = []\n",
        "  kw_extractor = yake.KeywordExtractor(top=20, stopwords=None)\n",
        "  keywords = kw_extractor.extract_keywords(' '.join(tweet_query))\n",
        "  for kw, v in keywords:\n",
        "    #print(\"Keyphrase: \",kw, \": score\", v)\n",
        "    for key in kw.split():\n",
        "      if(key not in tweet_keywords):\n",
        "        tweet_keywords.append(key)\n",
        "  \n",
        "  docs_preprocessed = []\n",
        "\n",
        "  total_documents = 0\n",
        "  path = '/content/drive/MyDrive/Tweelink_Dataset/Tweelink_Articles_Processed'\n",
        "  for filename in glob(os.path.join(path, '*')):\n",
        "    with open(os.path.join(os.getcwd(), filename), 'r', encoding = 'utf-8',errors = 'ignore') as f:\n",
        "      filename = os.path.basename(f.name)\n",
        "      data = json.load(f)\n",
        "      d_date = data[\"Date\"]\n",
        "      if(d_date==\"\" or d_date==\"Date\"):\n",
        "        continue\n",
        "      format = '%Y-%m-%d'\n",
        "  \n",
        "      d_present_date = datetime.datetime.strptime(d_date, format)\n",
        "  \n",
        "      if(str(d_present_date.date()) not in [str(u_present_date.date()), str(u_prev_date.date()), str(u_next_date.date())]):\n",
        "        continue\n",
        "    \n",
        "      docs_preprocessed.append({'Name':filename, 'Data':data})\n",
        "      total_documents+=1\n",
        "  \n",
        "  # Plain\n",
        "  relevant_articles_list_plain = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_query)\n",
        "  mean_average_precision_hashtag_plain = mean_average_precision(20, u_base_hashtag, u_time, relevant_articles_list_plain, docs_preprocessed)\n",
        "  global_average_mean_average_precision_plain.append(mean_average_precision_hashtag_plain)\n",
        "  mean_average_recall_hashtag_plain = mean_average_recall(20, u_base_hashtag, u_time, relevant_articles_list_plain, docs_preprocessed)\n",
        "  global_mean_average_recall_plain.append(mean_average_recall_hashtag_plain)\n",
        "  nDCG_hashtag_cs_count_plain = nDCG(u_base_hashtag, u_time, relevant_articles_list_plain, docs_preprocessed)\n",
        "  global_mean_average_ndcg_plain.append(nDCG_hashtag_cs_count_plain)\n",
        "\n",
        "\n",
        "  # keyword extractor\n",
        "  tweet_query_keyword_extractor = get_keyword_keywordextractor(df_query)\n",
        "  relevant_articles_list_keyword_extractor = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_query_keyword_extractor[:20])\n",
        "  mean_average_precision_hashtag_keyword_extractor = mean_average_precision(20, u_base_hashtag, u_time, relevant_articles_list_keyword_extractor, docs_preprocessed)\n",
        "  global_average_mean_average_precision_keyword_extractor.append(mean_average_precision_hashtag_keyword_extractor)\n",
        "  mean_average_recall_hashtag_keyword_extractor = mean_average_recall(20, u_base_hashtag, u_time, relevant_articles_list_keyword_extractor, docs_preprocessed)\n",
        "  global_mean_average_recall_keyword_extractor.append(mean_average_recall_hashtag_keyword_extractor)\n",
        "  nDCG_hashtag_cs_count_keyword_extractor = nDCG(u_base_hashtag, u_time, relevant_articles_list_keyword_extractor, docs_preprocessed)\n",
        "  global_mean_average_ndcg_keyword_extractor.append(nDCG_hashtag_cs_count_keyword_extractor)\n",
        "\n",
        "\n",
        "  # Rake\n",
        "  tweet_query_rake = get_rake_keywords(tweet_query)\n",
        "  relevant_articles_list_rake = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_query_rake)\n",
        "  mean_average_precision_hashtag_rake = mean_average_precision(20, u_base_hashtag, u_time, relevant_articles_list_rake, docs_preprocessed)\n",
        "  global_average_mean_average_precision_rake.append(mean_average_precision_hashtag_rake)\n",
        "  mean_average_recall_hashtag_rake = mean_average_recall(20, u_base_hashtag, u_time, relevant_articles_list_rake, docs_preprocessed)\n",
        "  global_mean_average_recall_rake.append(mean_average_recall_hashtag_rake)\n",
        "  nDCG_hashtag_cs_count_rake = nDCG(u_base_hashtag, u_time, relevant_articles_list_rake, docs_preprocessed)\n",
        "  global_mean_average_ndcg_rake.append(nDCG_hashtag_cs_count_rake)\n",
        "\n",
        "\n",
        "   # Yake\n",
        "  tweet_query_yake = get_yake_keywords(tweet_query)\n",
        "  relevant_articles_list_yake = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_query_yake)\n",
        "  mean_average_precision_hashtag_yake = mean_average_precision(20, u_base_hashtag, u_time, relevant_articles_list_yake, docs_preprocessed)\n",
        "  global_average_mean_average_precision_yake.append(mean_average_precision_hashtag_yake)\n",
        "  mean_average_recall_hashtag_yake = mean_average_recall(20, u_base_hashtag, u_time, relevant_articles_list_yake, docs_preprocessed)\n",
        "  global_mean_average_recall_yake.append(mean_average_recall_hashtag_yake)\n",
        "  nDCG_hashtag_cs_count_yake = nDCG(u_base_hashtag, u_time, relevant_articles_list_yake, docs_preprocessed)\n",
        "  global_mean_average_ndcg_yake.append(nDCG_hashtag_cs_count_yake)\n",
        "\n",
        "\n",
        "  # Text Rank\n",
        "  tweet_query_text_rank = get_text_rank_keywords(tweet_query)\n",
        "  relevant_articles_list_text_rank = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_query_text_rank)\n",
        "  mean_average_precision_hashtag_text_rank = mean_average_precision(20, u_base_hashtag, u_time, relevant_articles_list_text_rank, docs_preprocessed)\n",
        "  global_average_mean_average_precision_text_rank.append(mean_average_precision_hashtag_text_rank)\n",
        "  mean_average_recall_hashtag_text_rank = mean_average_recall(20, u_base_hashtag, u_time, relevant_articles_list_text_rank, docs_preprocessed)\n",
        "  global_mean_average_recall_text_rank.append(mean_average_recall_hashtag_text_rank)\n",
        "  nDCG_hashtag_cs_count_text_rank = nDCG(u_base_hashtag, u_time, relevant_articles_list_text_rank, docs_preprocessed)\n",
        "  global_mean_average_ndcg_text_rank.append(nDCG_hashtag_cs_count_text_rank)\n",
        "\n",
        "\n",
        "  # Keybert\n",
        "  tweet_query_keybert = get_keybert_keywords(tweet_query)\n",
        "  relevant_articles_list_keybert = find_relevant_documents_cosine_similarity_count_vectorizer(docs_preprocessed, tweet_query_keybert)\n",
        "  mean_average_precision_hashtag_keybert = mean_average_precision(20, u_base_hashtag, u_time, relevant_articles_list_text_rank, docs_preprocessed)\n",
        "  global_average_mean_average_precision_keybert.append(mean_average_precision_hashtag_keybert)\n",
        "  mean_average_recall_hashtag_keybert = mean_average_recall(20, u_base_hashtag, u_time, relevant_articles_list_keybert, docs_preprocessed)\n",
        "  global_mean_average_recall_keybert.append(mean_average_recall_hashtag_keybert)\n",
        "  nDCG_hashtag_cs_count_keybert = nDCG(u_base_hashtag, u_time, relevant_articles_list_keybert, docs_preprocessed)\n",
        "  global_mean_average_ndcg_keybert.append(nDCG_hashtag_cs_count_keybert)"
      ],
      "metadata": {
        "id": "Ik-Hd_WC892W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "1debb7e0-387b-4cca-e986-9e30b58d10e8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 9/30 [27:23<1:03:55, 182.65s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-5fb7a6642259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m   \u001b[0;31m# keyword extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m   \u001b[0mtweet_query_keyword_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_keyword_keywordextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m   \u001b[0mrelevant_articles_list_keyword_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_relevant_documents_cosine_similarity_count_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_preprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_query_keyword_extractor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0mmean_average_precision_hashtag_keyword_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_average_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_base_hashtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_articles_list_keyword_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_preprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-908d6674c9c0>\u001b[0m in \u001b[0;36mget_keyword_keywordextractor\u001b[0;34m(df_query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_keyword_keywordextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mkeyword_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_query\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtweet_query_keyword_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyword_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtweet_query_keyword_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-e43fa4b7f843>\u001b[0m in \u001b[0;36mkeyword_extractor\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    168\u001b[0m   \u001b[0;31m#print(vocabulary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m   \u001b[0mmaxdegree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m   \u001b[0mmax_degree_nodes_with_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mterm_frequency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmaxdegree\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m   \u001b[0mmaxfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_degree_nodes_with_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_average_mean_average_precision_plain = sum(global_average_mean_average_precision_plain)/len(global_average_mean_average_precision_plain)\n",
        "print(overall_average_mean_average_precision_plain)\n",
        "\n",
        "overall_mean_average_recall_plain = sum(global_mean_average_recall_plain)/len(global_mean_average_recall_plain)\n",
        "print(overall_mean_average_recall_plain)\n",
        "\n",
        "overall_mean_average_ndcg_plain = sum(global_mean_average_ndcg_plain)/len(global_mean_average_ndcg_plain)\n",
        "print(overall_mean_average_ndcg_plain)"
      ],
      "metadata": {
        "id": "d3i5WoKut-BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e8bf7bc-0272-42d7-b242-36e5827ef1d9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8124783907661344\n",
            "0.42305555555555563\n",
            "0.7753337593569972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_average_mean_average_precision_keyword_extractor = sum(global_average_mean_average_precision_keyword_extractor)/len(global_average_mean_average_precision_keyword_extractor)\n",
        "print(overall_average_mean_average_precision_keyword_extractor)\n",
        "\n",
        "overall_mean_average_recall_keyword_extractor = sum(global_mean_average_recall_keyword_extractor)/len(global_mean_average_recall_keyword_extractor)\n",
        "print(overall_mean_average_recall_keyword_extractor)\n",
        "\n",
        "overall_mean_average_ndcg_keyword_extractor = sum(global_mean_average_ndcg_keyword_extractor)/len(global_mean_average_ndcg_keyword_extractor)\n",
        "print(overall_mean_average_ndcg_keyword_extractor)"
      ],
      "metadata": {
        "id": "-GJmhs4gt_YD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4eeddb-93ec-4b11-dd2d-1bd30d4f0f9e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8790860452214848\n",
            "0.4493827160493827\n",
            "0.803335186105183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_average_mean_average_precision_rake = sum(global_average_mean_average_precision_rake)/len(global_average_mean_average_precision_rake)\n",
        "print(overall_average_mean_average_precision_rake)\n",
        "\n",
        "overall_mean_average_recall_rake = sum(global_mean_average_recall_rake)/len(global_mean_average_recall_rake)\n",
        "print(overall_mean_average_recall_rake)\n",
        "\n",
        "overall_mean_average_ndcg_rake = sum(global_mean_average_ndcg_rake)/len(global_mean_average_ndcg_rake)\n",
        "print(overall_mean_average_ndcg_rake)"
      ],
      "metadata": {
        "id": "yfRDY8U4uAqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332d5189-df1c-4e94-d871-432cd945cc3e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.30924786075799904\n",
            "0.14027777777777778\n",
            "0.25367922039171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_average_mean_average_precision_yake = sum(global_average_mean_average_precision_yake)/len(global_average_mean_average_precision_yake)\n",
        "print(overall_average_mean_average_precision_yake)\n",
        "\n",
        "overall_mean_average_recall_yake = sum(global_mean_average_recall_yake)/len(global_mean_average_recall_yake)\n",
        "print(overall_mean_average_recall_yake)\n",
        "\n",
        "overall_mean_average_ndcg_yake = sum(global_mean_average_ndcg_yake)/len(global_mean_average_ndcg_yake)\n",
        "print(overall_mean_average_ndcg_yake)"
      ],
      "metadata": {
        "id": "AARC7A4HuByA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3ac2f0-3f50-4cec-cb4b-99a952282cf6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9294677518261596\n",
            "0.4791358024691357\n",
            "0.8635828309915449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_average_mean_average_precision_text_rank = sum(global_average_mean_average_precision_text_rank)/len(global_average_mean_average_precision_text_rank)\n",
        "print(overall_average_mean_average_precision_text_rank)\n",
        "\n",
        "overall_mean_average_recall_text_rank = sum(global_mean_average_recall_text_rank)/len(global_mean_average_recall_text_rank)\n",
        "print(overall_mean_average_recall_text_rank)\n",
        "\n",
        "overall_mean_average_ndcg_text_rank = sum(global_mean_average_ndcg_text_rank)/len(global_mean_average_ndcg_text_rank)\n",
        "print(overall_mean_average_ndcg_text_rank)"
      ],
      "metadata": {
        "id": "IW_ocj3puC9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cf09e5-c246-47ea-af59-2dae5034911a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8853013496783461\n",
            "0.45342592592592595\n",
            "0.8501350784037831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_average_mean_average_precision_keybert = sum(global_average_mean_average_precision_keybert)/len(global_average_mean_average_precision_keybert)\n",
        "print(overall_average_mean_average_precision_keybert)\n",
        "\n",
        "overall_mean_average_recall_keybert = sum(global_mean_average_recall_keybert)/len(global_mean_average_recall_keybert)\n",
        "print(overall_mean_average_recall_keybert)\n",
        "\n",
        "overall_mean_average_ndcg_keybert = sum(global_mean_average_ndcg_keybert)/len(global_mean_average_ndcg_keybert)\n",
        "print(overall_mean_average_ndcg_keybert)"
      ],
      "metadata": {
        "id": "iNG8P0McuEey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84db8eca-d592-4183-ea15-dd033ff26d16"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8853013496783461\n",
            "0.3514197530864197\n",
            "0.607624949006746\n"
          ]
        }
      ]
    }
  ]
}